{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering for Implicit Feedback Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function for squared error with regularization\n",
    "\n",
    "Across all x (users) and y (items), find the values of u and i that minimize the summation below:\n",
    "\n",
    "$\\underset{x,y}min\\underset{u,i}\\sum \n",
    "c_{ui} (p_{ui} - x_u^Ty_i)^2 + \\lambda\n",
    "(\\underset u \\sum \\parallel x_u \\parallel ^2\n",
    "+\\underset u \\sum \\parallel y_i \\parallel ^2)$\n",
    "\n",
    "##### Where:\n",
    "\n",
    "$x_u$ is user vector,\n",
    "$y_i$ is item vector.\n",
    "\n",
    "$p_{ui} = 1$ if interaction, \n",
    "$p_{ui} = 0$ if no interaction.\n",
    "\n",
    "$c_{ui} = 1 + \\alpha * r_{ui}$, where\n",
    "$r_{ui}$ = # of interactions for a user-item pair, and $\\alpha$ determines our confidence levels.\n",
    "\n",
    "$\\lambda$ is regularization term.\n",
    "\n",
    "#### Explanation of cost function\n",
    "\n",
    "We take the squared error of our prediction and \n",
    "multiply by the confidence, and regularize our $x$ and $y$ vectors with $\\lambda$ to penalize overfitting. (larger values or smaller values?)\n",
    "\n",
    "$\\alpha$ allows us to influence our confidence levels. Clearly, our confidence increases when a producer samples the same artist multiple times, but by how much? $\\alpha$ determines how important multiple samples are.\n",
    "\n",
    "We add 1 so that non-interactions are not lost during the cost calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALS Algorithm\n",
    "\n",
    "However, we can't use the cost function above because of the size of the dataset. (m * n terms)\n",
    "\n",
    "Therefore we modify the cost function to Alternating Least Squares, which works by holding either user vectors or item vectors constant and calculating the global minimum, then alternating to the other vector.\n",
    "\n",
    "#### Compute user factors\n",
    "\n",
    "$x_u = (Y^T C^u Y + \\lambda I)^{-1}  Y^T C^u p(u)$\n",
    "\n",
    "##### Where:\n",
    "\n",
    "$Y$ is $n * f$ matrix of item-factors. \n",
    "\n",
    "$C^u$ is a $n*n$ diagonal matrix for user $u$ where $C^u_{ii} = c_{ui}$. This is our confidence matrix for n items.\n",
    "\n",
    "$p(u)$ is vector of preferences for user $u$.\n",
    "\n",
    "\n",
    "#### Recompute item factors\n",
    "\n",
    "$y_i = (X^TC^iX + \\lambda I)^-1 X^TC^ip(i)$\n",
    "\n",
    "##### Where:\n",
    "$X$ = $m * f$ matrix  of user_factors. \n",
    "\n",
    "$C^i$ is $m * m$ diagonal matrix for each item $i$ where $C_{uu}^i = c_{ui}$\n",
    "\n",
    "$p(i)$ is vector of preferences for item $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining recommendations\n",
    "\n",
    "If $\\hat{p}_{ui}$, the predicted preference of user $u$ at item $i$, is equal to $y_i^Tx_u$, we can substite our user_factor equation for $x_u$. This gives us:\n",
    "\n",
    "$\\hat{p}_{ui} =  y_i^T(Y^T C^u Y + \\lambda I)^{-1}  Y^T C^u p(u)$\n",
    "\n",
    "Denote $f*f$ matrix $(Y^T C^u Y + \\lambda I)^{-1}$ as $W^u$\n",
    "\n",
    "$W^u$ is considered the weight for user $u$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking Algorithm\n",
    "\n",
    "$\\overline{rank} = \\frac{\\sum_{u,i} r^t_{ui} * rank_{ui}}{\\sum_{u,i} r^t_{ui}}$\n",
    "\n",
    "#### where:\n",
    "$r^t_{ui}$ is the # of interactions for observations in the test set, and \n",
    "\n",
    "$rank_{ui}$ is how they ranked that item for that user. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They log_scaled their data, which makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Ranking Algorithm\n",
    "\n",
    "First, train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient()\n",
    "db = client.whosampled\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import implicit\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import scipy.sparse as sparse\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import os, sys\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"]=\"1\"\n",
    "\n",
    "import random\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "from src.turn_db_main_into_utility_matrix import from_mongo_collection_to_utility_matrix\n",
    "\n",
    "# Read in the data from the Mongo collection\n",
    "\n",
    "song_prod, artist_prod, df = from_mongo_collection_to_utility_matrix(db.main_redo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10.0/10 [00:02<00:00,  4.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# Using the parameters I found to be best from the Grid Search\n",
    "alpha = 20\n",
    "factors = 5\n",
    "iterations = 10\n",
    "\n",
    "model = implicit.als.AlternatingLeastSquares(factors=,iterations=iterations)\n",
    "\n",
    "# train the model on a sparse matrix of item/user/confidence weights\n",
    "sparse_artist_prod = csr_matrix(artist_prod)\n",
    "model.fit(sparse_artist_prod)\n",
    "\n",
    "# recommend items for a user\n",
    "sparse_prod_artist = sparse_artist_prod.T.tocsr()\n",
    "recommendations = model.recommend(0, sparse_prod_artist, 20, False)\n",
    "\n",
    "# find related items\n",
    "# related = model.similar_items(0, 20)\n",
    "\n",
    "# sim_users = model.similar_users(0, 10)\n",
    "\n",
    "# item_vecs = model.item_factors\n",
    "\n",
    "# user_vecs = model.user_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "new_song_year\n",
       "2019      0.127584\n",
       "2018      1.752243\n",
       "2017      3.614425\n",
       "2016      5.619121\n",
       "2015      8.069004\n",
       "2014     10.856848\n",
       "2013     14.537780\n",
       "2012     18.091127\n",
       "2011     21.231863\n",
       "2010     24.333238\n",
       "2009     26.670467\n",
       "2008     29.158353\n",
       "2007     31.654383\n",
       "2006     34.037759\n",
       "2005     36.783528\n",
       "2004     39.245626\n",
       "2003     41.907244\n",
       "2002     44.100601\n",
       "2001     46.638707\n",
       "2000     49.208030\n",
       "1999     52.375911\n",
       "1998     55.539721\n",
       "1997     58.646525\n",
       "1996     62.246020\n",
       "1995     65.739647\n",
       "1994     70.241731\n",
       "1993     74.872755\n",
       "1992     80.081979\n",
       "1991     84.847374\n",
       "1990     88.833693\n",
       "           ...    \n",
       "1982     98.813741\n",
       "1981     98.892463\n",
       "1980     98.946754\n",
       "1979     99.013261\n",
       "1978     99.082482\n",
       "1977     99.134058\n",
       "1976     99.201922\n",
       "1975     99.280644\n",
       "1974     99.363438\n",
       "1973     99.455732\n",
       "1972     99.523596\n",
       "1971     99.611819\n",
       "1970     99.686469\n",
       "1969     99.771978\n",
       "1968     99.857486\n",
       "1967     99.892775\n",
       "1966     99.929422\n",
       "1965     99.940280\n",
       "1964     99.944352\n",
       "1963     99.951138\n",
       "1962     99.963354\n",
       "1961     99.971497\n",
       "1960     99.980998\n",
       "1959     99.986427\n",
       "1958     99.989142\n",
       "1957     99.990499\n",
       "1956     99.994571\n",
       "1954     99.997285\n",
       "1878     99.998643\n",
       "1863    100.000000\n",
       "Name: URL, Length: 67, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What percentage of the data is 2018, 2017, etc?\n",
    "\n",
    "np.cumsum(df.groupby('new_song_year').count()['URL'].apply(\n",
    "    lambda x: (x / len(df)) * 100 ).sort_index(\n",
    "    ascending = False))\n",
    "\n",
    "# If we take the data past 2016-2019 as our test set, we get about 5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
