{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering for Implicit Feedback Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function for squared error with regularization\n",
    "\n",
    "Across all x (users) and y (items), find the values of u and i that minimize the summation below:\n",
    "\n",
    "$\\underset{x,y}min\\underset{u,i}\\sum \n",
    "c_{ui} (p_{ui} - x_u^Ty_i)^2 + \\lambda\n",
    "(\\underset u \\sum \\parallel x_u \\parallel ^2\n",
    "+\\underset u \\sum \\parallel y_i \\parallel ^2)$\n",
    "\n",
    "##### Where:\n",
    "\n",
    "$x_u$ is user vector,\n",
    "$y_i$ is item vector.\n",
    "\n",
    "$p_{ui} = 1$ if interaction, \n",
    "$p_{ui} = 0$ if no interaction.\n",
    "\n",
    "$c_{ui} = 1 + \\alpha * r_{ui}$, where\n",
    "$r_{ui}$ = # of interactions for a user-item pair, and $\\alpha$ determines our confidence levels.\n",
    "\n",
    "$\\lambda$ is regularization term.\n",
    "\n",
    "#### Explanation of cost function\n",
    "\n",
    "We take the squared error of our prediction and \n",
    "multiply by the confidence, and regularize our $x$ and $y$ vectors with $\\lambda$ to penalize overfitting. (larger values or smaller values?)\n",
    "\n",
    "$\\alpha$ allows us to influence our confidence levels. Clearly, our confidence increases when a producer samples the same artist multiple times, but by how much? $\\alpha$ determines how important multiple samples are.\n",
    "\n",
    "We add 1 so that non-interactions are not lost during the cost calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALS Algorithm\n",
    "\n",
    "However, we can't use the cost function above because of the size of the dataset. (m * n terms)\n",
    "\n",
    "Therefore we modify the cost function to Alternating Least Squares, which works by holding either user vectors or item vectors constant and calculating the global minimum, then alternating to the other vector.\n",
    "\n",
    "#### Compute user factors\n",
    "\n",
    "$x_u = (Y^T C^u Y + \\lambda I)^{-1}  Y^T C^u p(u)$\n",
    "\n",
    "##### Where:\n",
    "\n",
    "$Y$ is $n * f$ matrix of item-factors. \n",
    "\n",
    "$C^u$ is a $n*n$ diagonal matrix for user $u$ where $C^u_{ii} = c_{ui}$. This is our confidence matrix for n items.\n",
    "\n",
    "$p(u)$ is vector of preferences for user $u$.\n",
    "\n",
    "\n",
    "#### Recompute item factors\n",
    "\n",
    "$y_i = (X^TC^iX + \\lambda I)^-1 X^TC^ip(i)$\n",
    "\n",
    "##### Where:\n",
    "$X$ = $m * f$ matrix  of user_factors. \n",
    "\n",
    "$C^i$ is $m * m$ diagonal matrix for each item $i$ where $C_{uu}^i = c_{ui}$\n",
    "\n",
    "$p(i)$ is vector of preferences for item $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining recommendations\n",
    "\n",
    "If $\\hat{p}_{ui}$, the predicted preference of user $u$ at item $i$, is equal to $y_i^Tx_u$, we can substite our user_factor equation for $x_u$. This gives us:\n",
    "\n",
    "$\\hat{p}_{ui} =  y_i^T(Y^T C^u Y + \\lambda I)^{-1}  Y^T C^u p(u)$\n",
    "\n",
    "Denote $f*f$ matrix $(Y^T C^u Y + \\lambda I)^{-1}$ as $W^u$\n",
    "\n",
    "$W^u$ is considered the weight for user $u$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking Algorithm\n",
    "\n",
    "$\\overline{rank} = \\frac{\\sum_{u,i} r^t_{ui} * rank_{ui}}{\\sum_{u,i} r^t_{ui}}$\n",
    "\n",
    "#### where:\n",
    "$r^t_{ui}$ is the # of interactions for observations in the test set, and \n",
    "\n",
    "$rank_{ui}$ are the percentile ranking of each item for each user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "We can see that $\\sum_{u,i} r^t_{ui}$ is in both the numerator and the denominator. If $rank_{ui}$ was not in the numerator, $\\overline{rank}$ would simply equal 1. $rank_{ui}$ is the percentile ranking of each item for each user, such that the item most highly recommended has a $rank_{ui}$ of 0.00\\% and the item least recommended has a $rank_{ui}$ of 100.00\\%.\n",
    "\n",
    "Therefore, if the algorithm is correct, the low percentages will cancel out the high $r^t_{ui}$, making the $\\overline{rank}$ go towards 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They log_scaled their data, which makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Ranking Algorithm\n",
    "\n",
    "First, train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient()\n",
    "db = client.whosampled\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import implicit\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import scipy.sparse as sparse\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import os, sys\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"]=\"1\"\n",
    "\n",
    "import random\n",
    "\n",
    "from src.turn_db_main_into_utility_matrix import from_mongo_collection_to_utility_matrix\n",
    "\n",
    "# Read in the data from the Mongo collection\n",
    "\n",
    "song_prod, artist_prod, df = from_mongo_collection_to_utility_matrix(db.main_redo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15.0/15 [00:07<00:00,  2.87it/s]\n"
     ]
    }
   ],
   "source": [
    "# Using the parameters I found to be best from the Grid Search\n",
    "alpha = 20\n",
    "factors = 5\n",
    "iterations = 10\n",
    "\n",
    "model = implicit.als.AlternatingLeastSquares(factors=100,iterations=15)\n",
    "\n",
    "# train the model on a sparse matrix of item/user/confidence weights\n",
    "sparse_artist_prod = csr_matrix(artist_prod)\n",
    "model.fit(sparse_artist_prod)\n",
    "\n",
    "# recommend items for a user\n",
    "sparse_prod_artist = sparse_artist_prod.T.tocsr()\n",
    "recommendations = model.recommend(0, sparse_prod_artist, 20, False)\n",
    "\n",
    "# find related items\n",
    "# related = model.similar_items(0, 20)\n",
    "\n",
    "# sim_users = model.similar_users(0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1566672280043242"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_rank_ui_from_model(model):\n",
    "    \n",
    "    '''\n",
    "    Takes a fitted model and creates the rank_ui element of the rank\n",
    "    evaluation algorithm. These are every item's ranking of importance \n",
    "    for each user, with ranking as percentages. \n",
    "    '''\n",
    "    \n",
    "    item_vecs = model.item_factors\n",
    "    user_vecs = model.user_factors\n",
    "\n",
    "    #predictions is items, user\n",
    "    predictions = item_vecs.dot(user_vecs.T)\n",
    "\n",
    "    #We need it as u,i\n",
    "    preds_prods_artists = predictions.T\n",
    "    \n",
    "    #get the rank of each item for each user\n",
    "    order = np.flip(preds_prods_artists.argsort(axis = 1), axis = 1)\n",
    "    ranks = order.argsort(axis = 1)\n",
    "\n",
    "    #turn ranks into percentages\n",
    "    percentages = (ranks / ranks.shape[0]) * 100\n",
    "    return percentages\n",
    "\n",
    "def return_rank_score(percentages, r_ui):\n",
    "    \n",
    "    '''\n",
    "    Requires r_ui to be in form users, items.\n",
    "    '''\n",
    "    \n",
    "    numerator = r_ui * percentages\n",
    "\n",
    "    numer_summation = np.sum(np.sum(numerator))\n",
    "\n",
    "    denominator = np.sum(np.sum(prod_artists))\n",
    "\n",
    "    rank = numer_summation /denominator\n",
    "    return rank\n",
    "    \n",
    "percentages = create_rank_ui_from_model(model)\n",
    "rank = return_rank_score(percentages, artist_prod.T)\n",
    "rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11284, 8203)\n"
     ]
    }
   ],
   "source": [
    "# What if you just recommend the most popular?\n",
    "\n",
    "# Get most popular items in the mat\n",
    "samples_per_artist = artist_prod.sum(axis = 1)\n",
    "order = np.flip(samples_per_artist.values.argsort(), axis = 0)\n",
    "ranks = order.argsort()\n",
    "\n",
    "percentages = (ranks / len(samples_per_artist)) * 100\n",
    "\n",
    "#turn to series so as to concatenate into DF\n",
    "percentages = pd.Series(percentages)\n",
    "\n",
    "populars = pd.concat([percentages] * 11284, axis= 1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_pop = return_rank_score(populars, artist_prod.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_pop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on hidden 1-10%\n",
    "\n",
    "The ranking/ evaluation method for the above is to do this:\n",
    "\n",
    "1. Randomly take 10% of all of the interactions. Save their position in the matrix and values. Sum them. This is $\\sum_{u,i} r^t_{ui}$, for  $\\overline{rank} = \\frac{\\sum_{u,i} r^t_{ui} * rank_{ui}}{\\sum_{u,i} r^t_{ui}}$  \n",
    "\n",
    "2. Replace them with zeros. \n",
    "\n",
    "3. Train your model on this dataframe.\n",
    "\n",
    "4. We need the $rank_{ui}$ of our model for all $u, i$ in $r^t_{ui}$. Use this to compute rank.\n",
    "\n",
    "5. Get the $rank_{ui}$ of popularity for all $u, i$ in $r^t_{ui}$. This is popularity rank score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data into 2016 after and before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "new_song_year\n",
       "2019      0.127584\n",
       "2018      1.752243\n",
       "2017      3.614425\n",
       "2016      5.619121\n",
       "2015      8.069004\n",
       "2014     10.856848\n",
       "2013     14.537780\n",
       "2012     18.091127\n",
       "2011     21.231863\n",
       "2010     24.333238\n",
       "2009     26.670467\n",
       "2008     29.158353\n",
       "2007     31.654383\n",
       "2006     34.037759\n",
       "2005     36.783528\n",
       "2004     39.245626\n",
       "2003     41.907244\n",
       "2002     44.100601\n",
       "2001     46.638707\n",
       "2000     49.208030\n",
       "1999     52.375911\n",
       "1998     55.539721\n",
       "1997     58.646525\n",
       "1996     62.246020\n",
       "1995     65.739647\n",
       "1994     70.241731\n",
       "1993     74.872755\n",
       "1992     80.081979\n",
       "1991     84.847374\n",
       "1990     88.833693\n",
       "           ...    \n",
       "1982     98.813741\n",
       "1981     98.892463\n",
       "1980     98.946754\n",
       "1979     99.013261\n",
       "1978     99.082482\n",
       "1977     99.134058\n",
       "1976     99.201922\n",
       "1975     99.280644\n",
       "1974     99.363438\n",
       "1973     99.455732\n",
       "1972     99.523596\n",
       "1971     99.611819\n",
       "1970     99.686469\n",
       "1969     99.771978\n",
       "1968     99.857486\n",
       "1967     99.892775\n",
       "1966     99.929422\n",
       "1965     99.940280\n",
       "1964     99.944352\n",
       "1963     99.951138\n",
       "1962     99.963354\n",
       "1961     99.971497\n",
       "1960     99.980998\n",
       "1959     99.986427\n",
       "1958     99.989142\n",
       "1957     99.990499\n",
       "1956     99.994571\n",
       "1954     99.997285\n",
       "1878     99.998643\n",
       "1863    100.000000\n",
       "Name: URL, Length: 67, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What percentage of the data is 2018, 2017, etc?\n",
    "\n",
    "np.cumsum(df.groupby('new_song_year').count()['URL'].apply(\n",
    "    lambda x: (x / len(df)) * 100 ).sort_index(\n",
    "    ascending = False))\n",
    "\n",
    "# If we take the data past 2016-2019 as our test set, we get about 5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5945, 18)\n",
      "(67732, 18)\n"
     ]
    }
   ],
   "source": [
    "test = df[df.new_song_year > 2014]\n",
    "train = df[df.new_song_year < 2015]\n",
    "print(test.shape)\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1163, 1164, 1163]\n",
      "[544, 544, 544]\n"
     ]
    }
   ],
   "source": [
    "# Songs and users have to be in both, right?\n",
    "\n",
    "#get intersection of sampled_artist and song_producer from train and test set\n",
    "def get_intersection_of_train_and_test_set(train, test, column):\n",
    "    \n",
    "    '''\n",
    "    Takes train and test set and a column, and returns both filtered\n",
    "    to have values they both share\n",
    "    '''\n",
    "    test_column = set(test[column])\n",
    "    train_column = set(train[column])\n",
    "    both_columns = test_column.intersection(train_column)\n",
    "    print([len(test_column), len(train_column), len(both_columns)])\n",
    "    \n",
    "    test = test[test[column].isin(list(both_columns))]\n",
    "\n",
    "    train = train[train[column].isin(list(both_columns))]\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "train, test = get_intersection_of_train_and_test_set(\n",
    "    train, test, \"sampled_artist\")\n",
    "train, test = get_intersection_of_train_and_test_set(\n",
    "    train, test, \"new_song_producer\")\n",
    "\n",
    "#I needed to run this 3 times to work\n",
    "\n",
    "#I have 1163 artists, and 544 producers shared. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_prod_test = pd.crosstab(test_filtered.sampled_artist, columns=test_filtered.new_song_producer )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1381, 8653)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artist_prod_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_prod_train = pd.crosstab(train_filtered.sampled_artist, columns=train_filtered.new_song_producer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15.0/15 [00:03<00:00,  4.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# Using the parameters I found to be best from the Grid Search\n",
    "alpha = 20\n",
    "factors = 5\n",
    "iterations = 10\n",
    "\n",
    "model = implicit.als.AlternatingLeastSquares(factors=200,iterations=15)\n",
    "\n",
    "# training data for the model.\n",
    "sparse_artist_prod = csr_matrix(artist_prod_train)\n",
    "model.fit(sparse_artist_prod)\n",
    "\n",
    "# recommend items for a user\n",
    "# sparse_prod_artist = sparse_artist_prod.T.tocsr()\n",
    "# recommendations = model.recommend(0, sparse_prod_artist, 20, False)\n",
    "\n",
    "# find related items\n",
    "# related = model.similar_items(0, 20)\n",
    "\n",
    "# sim_users = model.similar_users(0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentages = create_rank_ui_from_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to coerce to DataFrame, shape must be (1456, 1381): given (8653, 1381)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-310-182f78db38bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_rank_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpercentages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martist_prod_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-160-07aa5f70de9d>\u001b[0m in \u001b[0;36mreturn_rank_score\u001b[0;34m(percentages, r_ui)\u001b[0m\n\u001b[1;32m     30\u001b[0m     '''\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mnumerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr_ui\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpercentages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mnumer_summation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(self, other, axis, level, fill_value)\u001b[0m\n\u001b[1;32m   2016\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2017\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2018\u001b[0;31m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_align_method_FRAME\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2019\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2020\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36m_align_method_FRAME\u001b[0;34m(left, right, axis)\u001b[0m\n\u001b[1;32m   1974\u001b[0m                                  \u001b[0;34m\"must be {req_shape}: given {given_shape}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1975\u001b[0m                                  .format(req_shape=left.shape,\n\u001b[0;32m-> 1976\u001b[0;31m                                          given_shape=right.shape))\n\u001b[0m\u001b[1;32m   1977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1978\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to coerce to DataFrame, shape must be (1456, 1381): given (8653, 1381)"
     ]
    }
   ],
   "source": [
    "rank = return_rank_score(percentages, artist_prod_test.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
